{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_sdc(data):\n",
    "    sdc_indices = np.argwhere(data['state/is_sdc'] == True)\n",
    "    sdc_indices = np.squeeze(sdc_indices, 0)\n",
    "    # print(sdc_indices)\n",
    "    data['sdc/current/x'] = np.expand_dims(data['state/current/x'][sdc_indices[0], sdc_indices[1]], 0)\n",
    "    data['sdc/current/y'] = np.expand_dims(data['state/current/y'][sdc_indices[0], sdc_indices[1]], 0)\n",
    "    data['sdc/current/z'] = np.expand_dims(data['state/current/z'][sdc_indices[0], sdc_indices[1]], 0)\n",
    "    data['sdc/current/bbox_yaw'] = np.expand_dims(data['state/current/bbox_yaw'][sdc_indices[0], sdc_indices[1]], 0)\n",
    "    return data\n",
    "    \n",
    "def _stack_field(data,times,field):\n",
    "    if field == 'type':\n",
    "        # [batch_size, num_agents]\n",
    "        fields = data['state/type']\n",
    "        # The `type` field's shape is different from other fields.  Broadcast it\n",
    "        # to have the same shape as another field.\n",
    "        x = _stack_field(data, times, field='x')\n",
    "        # [batch_size, num_agents, num_steps, 1]\n",
    "        fields = np.broadcast_to(fields[:, :, np.newaxis, np.newaxis], x.shape)\n",
    "    else:\n",
    "        # [batch_size, num_agents, num_steps]\n",
    "        fields = np.concatenate([data[f'state/{t}/{field}'] for t in times], axis=-1)\n",
    "        # [batch_size, num_agents, num_steps, 1]\n",
    "        fields = fields[:, :, :, np.newaxis]\n",
    "    return fields\n",
    "  \n",
    "def get_time(times):\n",
    "    num_steps = 0\n",
    "    if 'past' in times:\n",
    "      num_steps += 10\n",
    "    if 'current' in times:\n",
    "      num_steps += 1\n",
    "    if 'future' in times:\n",
    "      num_steps += 80\n",
    "    return num_steps\n",
    "\n",
    "def rotate_points_around_origin(x, y, angle):\n",
    "    translated_x = np.cos(angle) * x - np.sin(angle) * y\n",
    "    translated_y = np.sin(angle) * x + np.cos(angle) * y\n",
    "    \n",
    "    return translated_x, translated_y\n",
    "\n",
    "def prepare_for_sample(data, times):\n",
    "    x = _stack_field(data, times, 'x')\n",
    "    y = _stack_field(data, times, 'y')\n",
    "    z = _stack_field(data, times, 'z')\n",
    "    bbox_yaw = _stack_field(data, times, 'bbox_yaw')\n",
    "    length = _stack_field(data, times, 'length')\n",
    "    width = _stack_field(data, times, 'width')\n",
    "    agent_type = _stack_field(data, times, 'type')\n",
    "    valid = _stack_field(data, times, 'valid')\n",
    "    \n",
    "    sdc_x = data['sdc/current/x'][:, np.newaxis, np.newaxis, :]\n",
    "    sdc_y = data['sdc/current/y'][:, np.newaxis, np.newaxis, :]\n",
    "    sdc_z = data['sdc/current/z'][:, np.newaxis, np.newaxis, :]\n",
    "    x = x - sdc_x\n",
    "    y = y - sdc_y\n",
    "    z = z - sdc_z\n",
    "    \n",
    "    angle = np.pi / 2 - data['sdc/current/bbox_yaw'][:, np.newaxis, np.newaxis, :]\n",
    "    x, y = rotate_points_around_origin(x, y, angle)\n",
    "    x = np.cos(angle) * x - np.sin(angle) * y\n",
    "    y = np.sin(angle) * x + np.cos(angle) * y\n",
    "    bbox_yaw = bbox_yaw + angle\n",
    "    \n",
    "    return (x, y, z, bbox_yaw, width, length, agent_type, valid)\n",
    "  \n",
    "def sample_agent_points(prepared_data, points_per_side_length = 48, points_per_side_width = 16):\n",
    "    x, y, z, bbox_yaw, width, length, agent_type, valid = prepared_data\n",
    "    if points_per_side_length == 1:\n",
    "      step_x = 0.0\n",
    "    else:\n",
    "      step_x = 1.0 / (points_per_side_length - 1)\n",
    "    if points_per_side_width == 1:\n",
    "      step_y = 0.0\n",
    "    else:\n",
    "      step_y = 1.0 / (points_per_side_width - 1)\n",
    "    unit_x = []\n",
    "    unit_y = []\n",
    "    for xi in range(points_per_side_length):\n",
    "      for yi in range(points_per_side_width):\n",
    "        unit_x.append(xi * step_x - 0.5)\n",
    "        unit_y.append(yi * step_y - 0.5)\n",
    "\n",
    "    # Center unit_x and unit_y if there was only 1 point on those dimensions.\n",
    "    if points_per_side_length == 1:\n",
    "      unit_x = np.array(unit_x) + 0.5\n",
    "    if points_per_side_width == 1:\n",
    "      unit_y = np.array(unit_y) + 0.5\n",
    "\n",
    "    unit_x = np.array(unit_x, dtype = np.float32)\n",
    "    unit_y = np.array(unit_y, dtype = np.float32)\n",
    "\n",
    "    num_points = points_per_side_length * points_per_side_width\n",
    "\n",
    "    # Transform the unit square points to agent dimensions and coordinate frames.\n",
    "    sin_yaw = np.sin(bbox_yaw)\n",
    "    cos_yaw = np.cos(bbox_yaw)\n",
    "\n",
    "    # [..., num_points]\n",
    "    tx = cos_yaw * length * unit_x - sin_yaw * width * unit_y + x\n",
    "    ty = sin_yaw * length * unit_x + cos_yaw * width * unit_y + y\n",
    "    tz = np.broadcast_to(z, tx.shape)\n",
    "\n",
    "    agent_type = np.broadcast_to(agent_type, tx.shape)\n",
    "    valid = np.broadcast_to(valid, tx.shape)\n",
    "\n",
    "    return tx, ty, tz, agent_type, valid\n",
    "  \n",
    "def transform_to_image_coordinate(points_x, points_y, pixels_per_meter = 3.2, sdc_x_in_grid = 128, sdc_y_in_grid = 196):\n",
    "    points_x = np.round(points_x * pixels_per_meter) + sdc_x_in_grid\n",
    "    points_y = np.round(-points_y * pixels_per_meter) + sdc_y_in_grid\n",
    "    \n",
    "    point_is_in_fov = np.logical_and(\n",
    "      np.logical_and(np.greater_equal(points_x, 0), np.greater_equal(points_y, 0)),\n",
    "      np.logical_and(np.less(points_x, grid_width_cells),np.less(points_y, grid_height_cells)))\n",
    "    \n",
    "    return points_x, points_y, point_is_in_fov\n",
    "  \n",
    "def get_points_from_bbox(data, times = ['past']):\n",
    "    prepared_data  = prepare_for_sample(data, times)  #translate and rotate according to sdc\n",
    "    x, y, z, agent_type, agent_valid = sample_agent_points(prepared_data)\n",
    "    points_x_img, points_y_img, points_is_in_fov = transform_to_image_coordinate(x, y)\n",
    "    point_is_in_fov_and_valid = np.logical_and(points_is_in_fov, agent_valid.astype(bool))\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    time_stamp_list = []\n",
    "    for object_type in all_agent_type:\n",
    "        agent_type_matches = np.equal(agent_type, object_type)\n",
    "        should_keep_point = np.logical_and(point_is_in_fov_and_valid, agent_type_matches)\n",
    "        # should_keep_point = agent_type_matches\n",
    "        points_indices = np.argwhere(should_keep_point).astype(np.int32)\n",
    "        \n",
    "        # print(points_indices.shape)\n",
    "        x_real_coor = x[points_indices[:, 0], points_indices[:, 1], points_indices[:, 2], points_indices[:, 3]]\n",
    "        y_real_coor = y[points_indices[:, 0], points_indices[:, 1], points_indices[:, 2], points_indices[:, 3]]\n",
    "        z_real_coor = z[points_indices[:, 0], points_indices[:, 1], points_indices[:, 2], points_indices[:, 3]]\n",
    "        x_real_coor = np.expand_dims(x_real_coor, axis = -1)\n",
    "        y_real_coor = np.expand_dims(y_real_coor, axis = -1)\n",
    "        z_real_coor = np.expand_dims(z_real_coor, axis = -1)\n",
    "        time_stamp = np.expand_dims(points_indices[:, 2], -1)\n",
    "\n",
    "        x_list.append(x_real_coor)\n",
    "        y_list.append(y_real_coor)\n",
    "        z_list.append(z_real_coor)\n",
    "        time_stamp_list.append(time_stamp)\n",
    "    \n",
    "    return x_list, y_list, z_list, time_stamp_list\n",
    "  \n",
    "def get_features(points_x_with_type, points_y_with_type, points_z_with_type, time_stamp_with_type, batch_idx):\n",
    "    points_features_list = []\n",
    "    for idx, type in enumerate(range(1, 4)):\n",
    "        type_embedding = nn.Embedding(type_embedding_num, embedded_dim)\n",
    "        time_embedding = nn.Embedding(time_embedding_num_past, embedded_dim)\n",
    "        type = torch.LongTensor([type])\n",
    "        type_concat = type_embedding(type).repeat(points_x_with_type[idx].shape[0], 1)\n",
    "        # print(type)\n",
    "        # print(type_concat.shape)\n",
    "        x = torch.tensor(points_x_with_type[idx])\n",
    "        y = torch.tensor(points_y_with_type[idx])\n",
    "        z = torch.tensor(points_z_with_type[idx])\n",
    "        time = torch.LongTensor(time_stamp_with_type[idx]+1)\n",
    "        time_concat = time_embedding(time).squeeze(1)\n",
    "        # print(time_concat.shape)\n",
    "        batch_concat = torch.LongTensor([batch_idx]).repeat(points_x_with_type[idx].shape[0], 1)\n",
    "        # print(batch_concat.shape)\n",
    "        points_features_this_type = torch.cat((x, y, z, type_concat, time_concat, batch_concat), -1)\n",
    "        points_features_list.append(points_features_this_type)\n",
    "        # print(points_features_this_type.shape)\n",
    "        \n",
    "    points_features = torch.cat(points_features_list, 0)\n",
    "\n",
    "    return points_features\n",
    "\n",
    "class VoxelGenerator(object):\n",
    "    def __init__(self,\n",
    "                 grid_height_cells,\n",
    "                 grid_width_cells,\n",
    "                 max_num_points,\n",
    "                 max_voxels=20000):\n",
    "        # grid_size = torch.tensor([batch_size, grid_height_cells, grid_width_cells, 1], dtype = torch.int32)\n",
    "        self._max_num_points = max_num_points\n",
    "        self._max_voxels = max_voxels\n",
    "        # self._grid_size = grid_size\n",
    "        self.grid_height_cells = grid_height_cells\n",
    "        self.grid_width_cells = grid_width_cells\n",
    "        \n",
    "\n",
    "    def generate(self, points):\n",
    "        \"\"\"Generate voxels given points.\"\"\"\n",
    "        return points_to_voxel(points,\n",
    "                                self.grid_height_cells, \n",
    "                                self.grid_width_cells,\n",
    "                                self._max_num_points,\n",
    "                                self._max_voxels)\n",
    "\n",
    "def points_to_voxel(points,\n",
    "                    grid_height_cells,\n",
    "                    grid_width_cells,\n",
    "                    max_points=35,\n",
    "                    max_voxels=20000,\n",
    "                    pixels_per_meter=3.2,\n",
    "                    sdc_x_in_grid=128,\n",
    "                    sdc_y_in_grid=192):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        voxels: [M, max_points, ndim] float tensor. only contain points.\n",
    "        coordinates: [M, 4] int32 tensor.\n",
    "        num_points_per_voxel: [M] int32 tensor.\n",
    "    \"\"\"\n",
    "    batch_size = points[:, -1].max().int().item() + 1\n",
    "    voxelmap_shape = tuple(torch.tensor([batch_size, grid_height_cells, grid_width_cells, 1], dtype = torch.int32))\n",
    "    # don't create large array in jit(nopython=True) code.\n",
    "    num_points_per_voxel = torch.zeros((max_voxels, ), dtype=torch.int32)\n",
    "    coor_to_voxelidx = -torch.ones(voxelmap_shape, dtype=torch.int32)\n",
    "    voxels = torch.zeros((max_voxels, max_points, points.shape[-1]), dtype=points.dtype)\n",
    "    coors = torch.zeros((max_voxels, 4), dtype=torch.int32)\n",
    "    \n",
    "    voxel_num = _points_to_voxel_kernel(points,\n",
    "                                        voxelmap_shape,\n",
    "                                        num_points_per_voxel,\n",
    "                                        coor_to_voxelidx, voxels, coors,\n",
    "                                        max_points, max_voxels,\n",
    "                                        pixels_per_meter,\n",
    "                                        sdc_x_in_grid,\n",
    "                                        sdc_y_in_grid)\n",
    "    # print(coors)\n",
    "    coors = coors[:voxel_num]\n",
    "    voxels = voxels[:voxel_num]\n",
    "    num_points_per_voxel = num_points_per_voxel[:voxel_num]\n",
    "\n",
    "    return voxels, coors, num_points_per_voxel\n",
    "\n",
    "def _points_to_voxel_kernel(points,\n",
    "                            voxelmap_shape,\n",
    "                            num_points_per_voxel,\n",
    "                            coor_to_voxelidx,\n",
    "                            voxels,\n",
    "                            coors,\n",
    "                            max_points=35,\n",
    "                            max_voxels=20000,\n",
    "                            pixels_per_meter=3.2,\n",
    "                            sdc_x_in_grid=128,\n",
    "                            sdc_y_in_grid=192):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        voxels: Shape [M, max_points, ndim], only contain points.\n",
    "        coordinates: Shape [M, 4].  [batch_idx, x, y, z]\n",
    "        num_points_per_voxel: Shape [M].\n",
    "    \"\"\"\n",
    "    N = points.shape[0]\n",
    "\n",
    "    coor = torch.zeros((4, ), dtype=torch.int32)\n",
    "    voxel_num = 0\n",
    "    points_in_img_x = torch.round(points[:, 0] * pixels_per_meter) + sdc_x_in_grid\n",
    "    points_in_img_y = torch.round(-points[:, 1] * pixels_per_meter) + sdc_y_in_grid\n",
    "    batch_size = voxelmap_shape[0]\n",
    "    # print(batch_size)\n",
    "    \n",
    "    for batch_id in range(batch_size):\n",
    "        indices = points[:, -1] == batch_id\n",
    "        img_coor_x = points_in_img_x[indices]\n",
    "        img_coor_y = points_in_img_y[indices]\n",
    "        for i in tqdm(range(len(img_coor_x))):\n",
    "            coor[0] = batch_id\n",
    "            coor[1], coor[2] = img_coor_x[i], img_coor_y[i]\n",
    "            coor[3] = 0\n",
    "            voxelidx = coor_to_voxelidx[coor[0], coor[1], coor[2], coor[3]]\n",
    "            if voxelidx == -1:\n",
    "                voxelidx = voxel_num\n",
    "                if voxel_num >= max_voxels:\n",
    "                    continue\n",
    "                voxel_num += 1\n",
    "                coor_to_voxelidx[coor[0], coor[1], coor[2], coor[3]] = voxelidx\n",
    "                coors[voxelidx] = coor\n",
    "            num = num_points_per_voxel[voxelidx]\n",
    "            if num < max_points:\n",
    "                voxels[voxelidx, num] = points[i]\n",
    "                num_points_per_voxel[voxelidx] += 1\n",
    "    return voxel_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140750/140750 [00:09<00:00, 15265.75it/s]\n",
      "100%|██████████| 633389/633389 [00:39<00:00, 16057.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11233, 32, 36])\n",
      "torch.Size([11233, 4])\n",
      "torch.Size([11233])\n"
     ]
    }
   ],
   "source": [
    "points_per_side_length = 48\n",
    "points_per_side_width = 16\n",
    "grid_height_cells = 256\n",
    "grid_width_cells= 256\n",
    "pixels_per_meter = 3.2\n",
    "sdc_x_in_grid = 128\n",
    "sdc_y_in_grid = 196\n",
    "all_agent_type = [1, 2, 3]\n",
    "type_embedding_num = 4\n",
    "time_embedding_num_past = 11\n",
    "embedded_dim = 16\n",
    "\n",
    "with open('train_preprocessed_data/0.pkl', 'rb') as fp:\n",
    "    data_1 = pickle.load(fp)\n",
    "    \n",
    "with open('train_preprocessed_data/1.pkl', 'rb') as fp:\n",
    "    data_2 = pickle.load(fp)\n",
    "data_1 = add_sdc(data_1)\n",
    "data_2 = add_sdc(data_2)\n",
    "\n",
    "points_x_with_type_1, points_y_with_type_1, points_z_with_type_1, time_stamp_with_type_1 = get_points_from_bbox(data_1, ['past'])\n",
    "points_x_with_type_2, points_y_with_type_2, points_z_with_type_2, time_stamp_with_type_2 = get_points_from_bbox(data_2, ['past'])\n",
    "\n",
    "\n",
    "points_features_1 = get_features(points_x_with_type_1, points_y_with_type_1, points_z_with_type_1, time_stamp_with_type_1, batch_idx = 0)\n",
    "points_features_2 = get_features(points_x_with_type_2, points_y_with_type_2, points_z_with_type_2, time_stamp_with_type_2, batch_idx = 1)\n",
    "\n",
    "points_features = torch.cat((points_features_1, points_features_2), 0)\n",
    "voxel_generator = VoxelGenerator(grid_height_cells=256, grid_width_cells=256, max_num_points=32)\n",
    "voxels, coors, num_points_per_voxel = voxel_generator.generate(points_features)\n",
    "print(voxels.shape)\n",
    "print(coors.shape)\n",
    "print(num_points_per_voxel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4096, 32, 5]\n",
      "95\n",
      "[4096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cumm import tensorview as tv\n",
    "from spconv.utils import Point2VoxelCPU3d\n",
    "\n",
    "gen = Point2VoxelCPU3d(\n",
    "        vsize_xyz=[1/3.2, 1/3.2, 200],\n",
    "        coors_range_xyz=[-40, -20, -100, 40, 60, 100],\n",
    "        num_point_features=\n",
    "        5,  # here num_point_features must equal to pc.shape[1]\n",
    "        max_num_voxels=200000,\n",
    "        max_num_points_per_voxel=32)\n",
    "\n",
    "pc = np.random.uniform(-10, 10, size=[100000, 3])\n",
    "other_pc_feature = np.random.uniform(-1, 1, size=[100000, 2])\n",
    "pc_with_feature = np.concatenate([pc, other_pc_feature], axis=1)\n",
    "pc_tv = tv.from_numpy(pc_with_feature)\n",
    "voxels_tv, indices_tv, num_p_in_vx_tv = gen.point_to_voxel(pc_tv)\n",
    "\n",
    "print(voxels_tv.shape)\n",
    "print(indices_tv.numpy_view()[:, 1].max())\n",
    "print(num_p_in_vx_tv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PFNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 use_norm=True,\n",
    "                 last_layer=False):\n",
    "        super().__init__()\n",
    " \n",
    "        self.last_vfe = last_layer\n",
    "        self.use_norm = use_norm\n",
    "        if not self.last_vfe:\n",
    "            out_channels = out_channels // 2\n",
    " \n",
    "        if self.use_norm:\n",
    "            # 根据论文中，这是是简化版pointnet网络层的初始化\n",
    "            # 论文中使用的是 1x1 的卷积层完成这里的升维操作（理论上使用卷积的计算速度会更快）\n",
    "            # 输入的通道数是刚刚经过数据增强过后的点云特征，每个点云有10个特征，\n",
    "            # 输出的通道数是64\n",
    "            self.linear = nn.Linear(in_channels, out_channels, bias=False)\n",
    "            # 一维BN层\n",
    "            self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_channels, out_channels, bias=True)\n",
    " \n",
    "        self.part = 50000\n",
    " \n",
    "    def forward(self, inputs):\n",
    "        if inputs.shape[0] > self.part:\n",
    "            # nn.Linear performs randomly when batch size is too large\n",
    "            num_parts = inputs.shape[0] // self.part\n",
    "            part_linear_out = [self.linear(inputs[num_part * self.part:(num_part + 1) * self.part])\n",
    "                               for num_part in range(num_parts + 1)]\n",
    "            x = torch.cat(part_linear_out, dim=0)\n",
    "        else:\n",
    "            # x的维度由（M, 32, 10）升维成了（M, 32, 64）\n",
    "            x = self.linear(inputs)\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        # BatchNorm1d层:(M, 64, 32) --> (M, 32, 64)\n",
    "        # （pillars,num_point,channel）->(pillars,channel,num_points)\n",
    "        # 这里之所以变换维度，是因为BatchNorm1d在通道维度上进行,对于图像来说默认模式为[N,C,H*W],通道在第二个维度上\n",
    "        x = self.norm(x.permute(0, 2, 1)).permute(0, 2, 1) if self.use_norm else x\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        x = F.relu(x)\n",
    "        # 完成pointnet的最大池化操作，找出每个pillar中最能代表该pillar的点\n",
    "        # x_max shape ：（M, 1, 64）　\n",
    "        # print(x.shape)   #[M, ]\n",
    "        #####################################################################\n",
    "        x_max = torch.max(x, dim=1, keepdim=True)[0]\n",
    " \n",
    "        if self.last_vfe:\n",
    "            # 返回经过简化版pointnet处理pillar的结果\n",
    "            return x_max\n",
    "        else:\n",
    "            x_repeat = x_max.repeat(1, inputs.shape[1], 1)\n",
    "            # print('x', x.shape)\n",
    "            # print('x_repeat', x_repeat.shape)\n",
    "            x_concatenated = torch.cat([x, x_repeat], dim=2)\n",
    "            # print('x_concat', x_concatenated.shape)\n",
    "            return x_concatenated\n",
    "        #####################################################################\n",
    "        # return x\n",
    "            \n",
    "\n",
    "USE_NORM = True\n",
    "NUM_FILTERS = [64]\n",
    "voxel_size = [1/3.2, 1/3.2]\n",
    "point_cloud_range = [-40, -20, 40, 60]  #[x_min, y_min, x_max, y_max]\n",
    " \n",
    "class PillarVFE(nn.Module):\n",
    "    \"\"\"\n",
    "    model_cfg:NAME: PillarVFE\n",
    "                    WITH_DISTANCE: False\n",
    "                    USE_ABSLOTE_XYZ: True\n",
    "                    USE_NORM: True\n",
    "                    NUM_FILTERS: [64]\n",
    "    num_point_features:4\n",
    "    voxel_size:[0.16 0.16 4]\n",
    "    POINT_CLOUD_RANGE: [0, -39.68, -3, 69.12, 39.68, 1]\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, num_point_features, voxel_size, point_cloud_range, with_distance = True):\n",
    "        super().__init__()\n",
    " \n",
    "        self.use_norm = USE_NORM\n",
    "        num_point_features += 4\n",
    "        self.with_distance = with_distance\n",
    "        if self.with_distance:\n",
    "            num_point_features += 1\n",
    " \n",
    "        self.num_filters = NUM_FILTERS\n",
    "        assert len(self.num_filters) > 0\n",
    "        num_filters = [num_point_features] + list(self.num_filters)\n",
    " \n",
    "        pfn_layers = []\n",
    "        for i in range(len(num_filters) - 1):\n",
    "            in_filters = num_filters[i]\n",
    "            out_filters = num_filters[i + 1]\n",
    "            pfn_layers.append(\n",
    "                PFNLayer(in_filters, out_filters, self.use_norm, last_layer=(i >= len(num_filters) - 2))\n",
    "            )\n",
    "        # 加入线性层，将10维特征变为64维特征\n",
    "        self.pfn_layers = nn.ModuleList(pfn_layers)\n",
    " \n",
    "        self.voxel_x = voxel_size[0]\n",
    "        self.voxel_y = voxel_size[1]\n",
    "        self.x_offset = self.voxel_x / 2 + point_cloud_range[0]\n",
    "        self.y_offset = self.voxel_y / 2 + point_cloud_range[1]\n",
    " \n",
    "    def get_output_feature_dim(self):\n",
    "        return self.num_filters[-1]\n",
    " \n",
    "    def get_paddings_indicator(self, actual_num, max_num, axis=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            actual_num:每个voxel实际点的数量(M,)\n",
    "            max_num:voxel最大点的数量(32,)\n",
    "        Returns:\n",
    "            paddings_indicator:表明一个pillar中哪些是真实数据,哪些是填充的0数据\n",
    "        \"\"\"\n",
    "        # 扩展一个维度，使变为（M，1）\n",
    "        actual_num = torch.unsqueeze(actual_num, axis + 1)\n",
    "        # [1, 1]\n",
    "        max_num_shape = [1] * len(actual_num.shape)\n",
    "        # [1, -1]\n",
    "        max_num_shape[axis + 1] = -1\n",
    "        # (1,32)\n",
    "        max_num = torch.arange(max_num, dtype=torch.int, device=actual_num.device).view(max_num_shape)\n",
    "        # (M, 32)\n",
    "        paddings_indicator = actual_num.int() > max_num\n",
    "        return paddings_indicator\n",
    " \n",
    "    def forward(self, voxel_features, coords, num_points_per_voxel):\n",
    "        # 求每个pillar中所有点云的和 (M, 32, 3)->(M, 1, 3) 设置keepdim=True的，则保留原来的维度信息\n",
    "        # 然后在使用求和信息除以每个点云中有多少个点来求每个pillar中所有点云的平均值 points_mean shape：(M, 1, 3)\n",
    "        points_mean = voxel_features[:, :, :2].sum(dim=1, keepdim=True) / num_points_per_voxel.type_as(voxel_features).view(-1, 1, 1)\n",
    "        # 每个点云数据减去该点对应pillar的平均值得到差值 xc,yc,zc\n",
    "        f_cluster = voxel_features[:, :, :2] - points_mean\n",
    " \n",
    "        # 创建每个点云到该pillar的坐标中心点偏移量空数据 xp,yp\n",
    "        f_center = torch.zeros_like(voxel_features[:, :, :2])\n",
    "        #  coords是每个网格点的坐标，即[432, 496, 1]，需要乘以每个pillar的长宽得到点云数据中实际的长宽（单位米）\n",
    "        #  同时为了获得每个pillar的中心点坐标，还需要加上每个pillar长宽的一半得到中心点坐标\n",
    "        #  每个点的x、y、z减去对应pillar的坐标中心点，得到每个点到该点中心点的偏移量\n",
    "        f_center[:, :, 0] = voxel_features[:, :, 0] - (\n",
    "                coords[:, 0].to(voxel_features.dtype).unsqueeze(1) * self.voxel_x + self.x_offset)\n",
    "        f_center[:, :, 1] = voxel_features[:, :, 1] - (\n",
    "                coords[:, 1].to(voxel_features.dtype).unsqueeze(1) * self.voxel_y + self.y_offset) \n",
    " \n",
    "        # if self.use_absolute_xyz:\n",
    "        features = [voxel_features[..., :-1], f_cluster, f_center]   #last dim is batch_idx\n",
    "        # else:\n",
    "        #     features = [voxel_features[..., 3:], f_cluster, f_center]\n",
    "        # 如果使用距离信息\n",
    "        if self.with_distance:\n",
    "            # torch.norm的第一个2指的是求2范数，第二个2是在第三维度求范数\n",
    "            points_dist = torch.norm(voxel_features[:, :, :3], 2, 2, keepdim=True)\n",
    "            features.append(points_dist)\n",
    "        # 将特征在最后一维度拼接 得到维度为（M，32,10）的张量\n",
    "        features = torch.cat(features, dim=-1)\n",
    "        # 每个pillar中点云的最大数量\n",
    "        voxel_count = features.shape[1]\n",
    "        \"\"\"\n",
    "        由于在生成每个pillar中,不满足最大32个点的pillar会存在由0填充的数据,\n",
    "        而刚才上面的计算中，会导致这些\n",
    "        由0填充的数据在计算出现xc,yc,zc和xp,yp,zp出现数值,\n",
    "        所以需要将这个被填充的数据的这些数值清0,\n",
    "        因此使用get_paddings_indicator计算features中哪些是需要被保留真实数据和需要被置0的填充数据\n",
    "        \"\"\"\n",
    "        # 得到mask维度是（M， 32）\n",
    "        # mask中指名了每个pillar中哪些是需要被保留的数据\n",
    "        mask = self.get_paddings_indicator(num_points_per_voxel, voxel_count, axis=0)\n",
    "        # （M， 32）->(M, 32, 1)\n",
    "        mask = torch.unsqueeze(mask, -1).type_as(voxel_features)\n",
    "        # 将feature中被填充数据的所有特征置0\n",
    "        features *= mask\n",
    " \n",
    "        for pfn in self.pfn_layers:\n",
    "            features = pfn(features)\n",
    "        # (M, 64), 每个pillar抽象出一个64维特征\n",
    "        features = features.squeeze()\n",
    "        pillar_features = features\n",
    "        return pillar_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12039, 64])\n"
     ]
    }
   ],
   "source": [
    "pillar_VFE = PillarVFE(voxels.shape[-1]-1, voxel_size, point_cloud_range, True)\n",
    "pillar_features = pillar_VFE.forward(voxels, coors, num_points_per_voxel)\n",
    "print(pillar_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BEV_FEATURES = 64\n",
    "class PointPillarScatter(nn.Module):\n",
    "    def __init__(self, grid_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_bev_features = NUM_BEV_FEATURES  # 64\n",
    "        self.nx, self.ny, self.nz = grid_size  # [432,496,1]\n",
    "        assert self.nz == 1\n",
    " \n",
    "    def forward(self, pillar_features, coors):\n",
    "        # 拿到经过前面pointnet处理过后的pillar数据和每个pillar所在点云中的坐标位置\n",
    "        # pillar_features 维度 （M， 64）\n",
    "        # coords 维度 （M， 4）\n",
    " \n",
    "        # 将转换成为伪图像的数据存在到该列表中\n",
    "        batch_spatial_features = []\n",
    "        batch_size = coors[:, 0].max().int().item() + 1\n",
    "        print(batch_size)\n",
    "        # for batch_idx in range(batch_size):\n",
    "            \n",
    "        for batch_idx in range(batch_size):\n",
    "            # 创建一个空间坐标所有用来接受pillar中的数据\n",
    "            # self.num_bev_features是64\n",
    "            # self.nz * self.nx * self.ny是生成的空间坐标索引 [496, 432, 1]的乘积\n",
    "            # spatial_feature 维度 (64,214272)\n",
    "            spatial_feature = torch.zeros((self.nx, self.ny, self.num_bev_features),dtype=pillar_features.dtype,device=pillar_features.device)  # (64,214272)-->1x432x496=214272\n",
    " \n",
    "            # 从coords[:, 0]取出该batch_idx的数据mask\n",
    "            batch_mask = coors[:, 0] == batch_idx\n",
    "            # 根据mask提取坐标\n",
    "            this_coors = coors[batch_mask, :].type(torch.long)\n",
    "            # this_coords中存储的坐标是z,y和x的形式,且只有一层，因此计算索引的方式如下\n",
    "            # 平铺后需要计算前面有多少个pillar 一直到当前pillar的索引\n",
    "\n",
    "            pillars = pillar_features[batch_mask, :]\n",
    "            spatial_feature[this_coors[:, 1], this_coors[:, 2], :] = pillars\n",
    "            # 将空间特征加入list,每个元素为(64, 214272)\n",
    "            batch_spatial_features.append(spatial_feature)\n",
    "\n",
    "        batch_spatial_features = torch.stack(batch_spatial_features, 0)\n",
    " \n",
    "        return batch_spatial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([2, 256, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "pillar_scatter = PointPillarScatter([grid_height_cells, grid_width_cells, 1])\n",
    "batch_spatial_features = pillar_scatter.forward(pillar_features, coors)\n",
    "print(batch_spatial_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_points_around_origin(x, y, angle):\n",
    "    translated_x = np.cos(angle) * x - np.sin(angle) * y\n",
    "    translated_y = np.sin(angle) * x + np.cos(angle) * y\n",
    "    \n",
    "    return translated_x, translated_y\n",
    "    \n",
    "\n",
    "def transform_roadgraph(data):\n",
    "    sdc_xyz = np.stack((data['sdc/current/x'], data['sdc/current/y'], data['sdc/current/z']), 2)\n",
    "    # sdc_xyz = sdc_xyz[:, :, np.newaxis, :]\n",
    "    \n",
    "    #translate\n",
    "    rg_points = data['roadgraph_samples/xyz'] - sdc_xyz\n",
    "    rg_x, rg_y = rg_points[..., 0], rg_points[..., 1]\n",
    "    \n",
    "    #rotate\n",
    "    angle = np.pi / 2 - data['sdc/current/bbox_yaw']\n",
    "    rg_x, rg_y = rotate_points_around_origin(rg_x, rg_y, angle)\n",
    "    \n",
    "    return rg_x, rg_y, rg_points[..., 2]\n",
    "\n",
    "def process_roadgraph(data):\n",
    "    rg_x, rg_y, rg_z = transform_roadgraph(data_1)\n",
    "    img_rg_x, img_rg_y, points_is_in_fov_rg = transform_to_image_coordinate(rg_x, rg_y)\n",
    "    \n",
    "    return rg_x, rg_y, points_is_in_fov_rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19562)\n",
      "(1, 19562)\n",
      "torch.Size([4885, 3])\n"
     ]
    }
   ],
   "source": [
    "rg_x_1, rg_y_1, points_is_in_fov_rg_1 = process_roadgraph(data_1)\n",
    "print(rg_x_1.shape)\n",
    "print(points_is_in_fov_rg_1.shape)\n",
    "\n",
    "points_indices = np.argwhere(points_is_in_fov_rg_1).astype(np.int32)\n",
    "rg_x_1 = rg_x_1[points_indices[:, 0], points_indices[:, 1]]\n",
    "rg_y_1 = rg_y_1[points_indices[:, 0], points_indices[:, 1]]\n",
    "\n",
    "rg_x_1 = torch.from_numpy(rg_x_1[..., np.newaxis])\n",
    "rg_y_1 = torch.from_numpy(rg_y_1[..., np.newaxis])\n",
    "batch_idx = 0\n",
    "batch_concat = torch.LongTensor([batch_idx+1]).repeat(rg_x_1.shape[0], 1)\n",
    "\n",
    "points_features_rg = torch.cat((rg_x_1, rg_y_1, batch_concat), -1)\n",
    "print(points_features_rg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 4885/4885 [00:00<00:00, 12259.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4662, 32, 3])\n",
      "torch.Size([4662, 4])\n",
      "torch.Size([4662])\n"
     ]
    }
   ],
   "source": [
    "voxel_generator_rg = VoxelGenerator(grid_height_cells=256, grid_width_cells=256, max_num_points=32)\n",
    "voxels_rg, coors_rg, num_points_per_voxel_rg = voxel_generator_rg.generate(points_features_rg)\n",
    "print(voxels_rg.shape)\n",
    "print(coors_rg.shape)\n",
    "print(num_points_per_voxel_rg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4662, 64])\n"
     ]
    }
   ],
   "source": [
    "pillar_VFE_rg = PillarVFE(2, voxel_size, point_cloud_range, False)\n",
    "pillar_features_rg = pillar_VFE_rg.forward(voxels_rg, coors_rg, num_points_per_voxel_rg)\n",
    "print(pillar_features_rg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42404073a09336142c28aaa40c0b46b47e795c1c799c1c07e6e4e8fecdbb5963"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('PCDet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
